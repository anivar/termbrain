# Gemini Integration Example

This example shows how to integrate TermBrain with Google's Gemini API for AI-powered command analysis, code generation, and intelligent terminal assistance.

## Features

- **Multi-Modal Analysis**: Leverage Gemini's vision capabilities for screenshot analysis
- **Code Generation**: Generate scripts and commands based on natural language
- **Contextual Help**: Get help based on your current working environment
- **Performance Analysis**: Analyze command performance and resource usage
- **Security Scanning**: Identify potential security issues in commands

## Setup

### 1. Install Dependencies

```bash
cd examples/gemini_integration
cargo build
```

### 2. Configure API Access

Create a `.env` file:

```bash
# Google Gemini API
GOOGLE_API_KEY=your_gemini_api_key_here

# Optional: Configure model preferences
GEMINI_MODEL=gemini-1.5-pro-latest
GEMINI_TEMPERATURE=0.2
GEMINI_MAX_OUTPUT_TOKENS=8192

# Safety settings
GEMINI_SAFETY_HARASSMENT=BLOCK_MEDIUM_AND_ABOVE
GEMINI_SAFETY_HATE=BLOCK_MEDIUM_AND_ABOVE
GEMINI_SAFETY_SEXUAL=BLOCK_MEDIUM_AND_ABOVE
GEMINI_SAFETY_DANGEROUS=BLOCK_MEDIUM_AND_ABOVE

# TermBrain Integration  
TERMBRAIN_DATA_DIR=/path/to/your/termbrain/data
```

### 3. Authentication Setup

```bash
# Set up Google Cloud credentials (alternative to API key)
gcloud auth application-default login

# Or use service account key
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
```

## Usage Examples

### Basic Command Analysis

```bash
# Analyze complex command with context
./gemini-analyzer analyze "kubectl get pods -o jsonpath='{.items[*].status.phase}' | tr ' ' '\n' | sort | uniq -c"

# Output:
# 🔍 Gemini Analysis
# 
# This is a sophisticated Kubernetes command that analyzes pod states:
# 
# 1. `kubectl get pods -o jsonpath='{.items[*].status.phase}'`
#    - Retrieves all pods and extracts just their phase status
#    - Uses JSONPath to navigate the API response structure
# 
# 2. `tr ' ' '\n'` - Converts spaces to newlines (one phase per line)
# 
# 3. `sort` - Alphabetically sorts the phase names
# 
# 4. `uniq -c` - Counts occurrences of each unique phase
# 
# 📊 **Result**: Shows count of pods in each state (Running, Pending, Failed, etc.)
# 
# 🚀 **Advanced Alternatives**:
# - `kubectl get pods --field-selector=status.phase=Running --no-headers | wc -l`
# - `kubectl top pods` for resource usage
# - Use `jq` for more readable JSON processing:
#   `kubectl get pods -o json | jq '.items | group_by(.status.phase) | map({phase: .[0].status.phase, count: length})'`
```

### Code Generation

```bash
# Generate a monitoring script
./gemini-analyzer generate --task "Create a bash script that monitors Docker container health and sends alerts"

# Output:
# 🤖 Generated Script: Docker Health Monitor
# 
# ```bash
# #!/bin/bash
# # Docker Container Health Monitor
# # Generated by Gemini AI based on your requirements
# 
# set -euo pipefail
# 
# # Configuration
# ALERT_EMAIL="admin@company.com"
# SLACK_WEBHOOK_URL="https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
# CHECK_INTERVAL=30
# 
# # Colors for output
# RED='\033[0;31m'
# GREEN='\033[0;32m'
# YELLOW='\033[1;33m'
# NC='\033[0m'
# 
# log() {
#     echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
# }
# 
# check_container_health() {
#     local container_id=$1
#     local container_name=$2
#     
#     # Get container status
#     local status=$(docker inspect --format='{{.State.Status}}' "$container_id")
#     local health=$(docker inspect --format='{{.State.Health.Status}}' "$container_id" 2>/dev/null || echo "none")
#     
#     if [[ "$status" != "running" ]]; then
#         send_alert "CRITICAL" "$container_name" "Container is $status"
#         return 1
#     elif [[ "$health" == "unhealthy" ]]; then
#         send_alert "WARNING" "$container_name" "Container health check failed"
#         return 1
#     fi
#     
#     return 0
# }
# 
# send_alert() {
#     local severity=$1
#     local container=$2
#     local message=$3
#     
#     log "${RED}[$severity]${NC} $container: $message"
#     
#     # Send email alert
#     if command -v mail >/dev/null 2>&1; then
#         echo "$message" | mail -s "[$severity] Docker Alert: $container" "$ALERT_EMAIL"
#     fi
#     
#     # Send Slack notification
#     if [[ -n "$SLACK_WEBHOOK_URL" ]]; then
#         curl -s -X POST -H 'Content-type: application/json' \
#             --data "{\"text\":\"🚨 [$severity] $container: $message\"}" \
#             "$SLACK_WEBHOOK_URL"
#     fi
# }
# 
# main() {
#     log "Starting Docker health monitor..."
#     
#     while true; do
#         # Get all running containers
#         while IFS= read -r line; do
#             container_id=$(echo "$line" | cut -d' ' -f1)
#             container_name=$(echo "$line" | cut -d' ' -f2-)
#             
#             if check_container_health "$container_id" "$container_name"; then
#                 log "${GREEN}✓${NC} $container_name is healthy"
#             fi
#             
#         done < <(docker ps --format "{{.ID}} {{.Names}}")
#         
#         sleep "$CHECK_INTERVAL"
#     done
# }
# 
# # Handle signals
# trap 'log "Shutting down health monitor..."; exit 0' SIGINT SIGTERM
# 
# main "$@"
# ```
# 
# 💡 **Usage Instructions**:
# 1. Save as `docker-health-monitor.sh`
# 2. Make executable: `chmod +x docker-health-monitor.sh`
# 3. Configure email and Slack settings
# 4. Run: `./docker-health-monitor.sh`
# 5. For background: `nohup ./docker-health-monitor.sh &`
# 
# 🔧 **Customization Options**:
# - Adjust CHECK_INTERVAL for frequency
# - Add more notification channels (Discord, Teams, etc.)
# - Include CPU/memory thresholds
# - Add container restart logic
# - Integrate with logging systems
```

### Multi-Modal Analysis (Screenshots)

```bash
# Analyze a terminal screenshot for issues
./gemini-analyzer vision --image terminal_error.png --context "Build failure"

# Output:
# 👁️ Gemini Vision Analysis
# 
# I can see a Rust compilation error in your terminal:
# 
# 🔍 **Error Analysis**:
# - **File**: src/main.rs:42:5
# - **Error**: `cannot find function 'proces_data' in this scope`
# - **Issue**: Typo in function name (missing 's')
# 
# 🛠️ **Quick Fix**:
# ```rust
# // Change line 42 from:
# proces_data(input);
# 
# // To:
# process_data(input);
# ```
# 
# 📝 **Additional Observations**:
# - Your terminal shows good development practices (cargo check before build)
# - Consider using `cargo clippy` for additional lint suggestions
# - The error is in a common pattern - function calls in main()
# 
# 💡 **Prevention Tips**:
# - Use IDE with Rust language server for real-time error detection
# - Enable rust-analyzer for better autocomplete
# - Consider adding unit tests to catch such issues early
```

### Performance Analysis

```bash
# Analyze command performance patterns
./gemini-analyzer performance --timeframe "last 24 hours"

# Output:
# ⚡ Performance Analysis Report
# 
# 📊 **Command Execution Times** (last 24 hours):
# 
# **Slowest Commands**:
# 1. `cargo build --release` - avg: 47.3s, runs: 8
# 2. `docker build .` - avg: 23.1s, runs: 12  
# 3. `npm install` - avg: 18.7s, runs: 3
# 4. `git clone large-repo` - avg: 15.2s, runs: 2
# 5. `find / -name "*.log"` - avg: 8.9s, runs: 4
# 
# **Performance Insights**:
# - Your Rust builds are slower than optimal
# - Docker builds could benefit from layer caching
# - Large repository clones suggest network bottleneck
# 
# 🚀 **Optimization Recommendations**:
# 
# **Rust Build Optimization**:
# ```toml
# # Add to Cargo.toml
# [profile.dev]
# debug = 1  # Faster linking
# incremental = true
# 
# [profile.release]
# lto = "thin"  # Better optimization
# codegen-units = 1
# ```
# 
# **Docker Build Optimization**:
# ```dockerfile
# # Use multi-stage builds
# FROM rust:1.70 as builder
# WORKDIR /app
# COPY Cargo.toml Cargo.lock ./
# RUN cargo build --release --dependencies-only
# COPY src ./src
# RUN cargo build --release
# 
# FROM debian:bullseye-slim
# COPY --from=builder /app/target/release/app /usr/local/bin/
# ```
# 
# **System-Level Optimizations**:
# - Enable Docker BuildKit: `export DOCKER_BUILDKIT=1`
# - Use `cargo-cache` to clean up build artifacts
# - Consider `mold` linker for faster Rust linking
# - Add SSD if using HDD for development
# 
# 📈 **Projected Improvements**:
# - Rust builds: 30-50% faster with incremental compilation
# - Docker builds: 60% faster with proper caching
# - Overall development velocity: +25%
```

### Security Analysis

```bash
# Scan recent commands for security issues
./gemini-analyzer security --scan-depth deep

# Output:
# 🔐 Security Analysis Report
# 
# ⚠️ **Security Issues Found**:
# 
# **HIGH RISK**:
# 1. **Exposed Credentials** (2 instances)
#    - `curl -H "Authorization: Bearer sk-abc123..."` 
#    - `docker run -e API_KEY=secret123 myapp`
#    - **Risk**: API keys in shell history
#    - **Fix**: Use environment files or secret management
# 
# 2. **Privileged Container Execution** (1 instance)
#    - `docker run --privileged ubuntu`
#    - **Risk**: Full host access from container
#    - **Fix**: Use specific capabilities instead
# 
# **MEDIUM RISK**:
# 1. **Unverified Downloads** (3 instances)
#    - `curl https://install.sh | bash`
#    - **Risk**: Executing untrusted code
#    - **Fix**: Download, inspect, then execute
# 
# 2. **Wildcard File Operations** (2 instances)
#    - `rm -rf *`
#    - **Risk**: Accidental data deletion
#    - **Fix**: Use specific file patterns
# 
# **LOW RISK**:
# 1. **Insecure HTTP** (4 instances)
#    - Various curl commands using HTTP instead of HTTPS
#    - **Fix**: Always use HTTPS when available
# 
# 🛡️ **Security Recommendations**:
# 
# **Immediate Actions**:
# ```bash
# # 1. Clear sensitive history
# history -c
# history -w
# 
# # 2. Use environment files for secrets
# echo "API_KEY=your_key_here" > .env
# docker run --env-file .env myapp
# 
# # 3. Set up git-secrets to prevent credential commits
# git secrets --install
# git secrets --register-aws
# ```
# 
# **Long-term Improvements**:
# - Implement secret management (HashiCorp Vault, AWS Secrets Manager)
# - Use signed container images
# - Enable shell history encryption
# - Set up security scanning in CI/CD
# - Regular security audits of command patterns
# 
# **Security Score**: 6.5/10
# *Needs improvement in credential handling and privilege management*
```

### Contextual Environment Help

```bash
# Get help based on current environment
./gemini-analyzer help --context auto

# Output:
# 🤖 Contextual Help - Kubernetes Development Environment
# 
# **Environment Detected**: 
# - Kubernetes cluster (minikube)
# - Current namespace: development
# - 3 deployments running
# - Working directory: /home/user/k8s-project
# 
# **Relevant Commands for Your Current Context**:
# 
# **Immediate Actions**:
# ```bash
# # Check your current deployments
# kubectl get deployments -n development
# 
# # View recent pod logs
# kubectl logs -f deployment/api-server -n development
# 
# # Port forward for local testing
# kubectl port-forward svc/api-server 8080:80 -n development
# ```
# 
# **Development Workflow**:
# ```bash
# # Apply configuration changes
# kubectl apply -f k8s/ -n development
# 
# # Check rollout status
# kubectl rollout status deployment/api-server -n development
# 
# # Scale for testing
# kubectl scale deployment api-server --replicas=2 -n development
# ```
# 
# **Debugging Commands**:
# ```bash
# # Describe problematic pods
# kubectl describe pod $(kubectl get pods -l app=api-server -o jsonpath='{.items[0].metadata.name}')
# 
# # Get events in namespace
# kubectl get events -n development --sort-by='.lastTimestamp'
# 
# # Execute commands in pod
# kubectl exec -it deployment/api-server -- /bin/bash
# ```
# 
# **Resource Monitoring**:
# ```bash
# # Resource usage
# kubectl top pods -n development
# kubectl top nodes
# 
# # Resource limits and requests
# kubectl describe nodes | grep -A 5 "Allocated resources"
# ```
# 
# 💡 **Pro Tips for Your Setup**:
# - Use `kubectl config set-context --current --namespace=development` to avoid typing `-n development`
# - Set up aliases: `alias k=kubectl`
# - Install `kubectx` and `kubens` for easier context switching
# - Use `stern` for better log aggregation: `stern api-server -n development`
# 
# **Detected Issues**:
# - Your api-server pod has been restarting (3 restarts in last hour)
# - Memory usage is at 85% - consider increasing limits
# - No health checks configured - add readiness/liveness probes
```

## Advanced Integration Features

### Custom Workflows with Gemini

```rust
// examples/gemini_integration/src/workflow_generator.rs
use serde::{Deserialize, Serialize};

#[derive(Debug, Serialize, Deserialize)]
pub struct WorkflowRequest {
    pub goal: String,
    pub constraints: Vec<String>,
    pub environment: Environment,
    pub preferences: UserPreferences,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Environment {
    pub os: String,
    pub shell: String,
    pub available_tools: Vec<String>,
    pub current_directory: String,
    pub git_repository: Option<String>,
}

impl GeminiWorkflowGenerator {
    pub async fn generate_workflow(&self, request: &WorkflowRequest) -> Result<Workflow> {
        let prompt = self.build_workflow_prompt(request);
        let response = self.client.generate_content(&prompt).await?;
        
        // Parse the generated workflow
        let workflow = self.parse_workflow_response(&response)?;
        
        // Validate the workflow
        self.validate_workflow(&workflow, &request.environment)?;
        
        Ok(workflow)
    }
    
    fn build_workflow_prompt(&self, request: &WorkflowRequest) -> String {
        format!(
            r#"Generate a complete workflow to achieve this goal: "{}"

Environment:
- OS: {}
- Shell: {}
- Available tools: {}
- Current directory: {}
- Git repository: {}

Constraints:
{}

Please provide:
1. Step-by-step commands with explanations
2. Error handling and validation steps
3. Rollback procedures if applicable
4. Prerequisites and dependencies
5. Expected outputs and verification steps
6. Security considerations

Format as a structured workflow with clear sections."#,
            request.goal,
            request.environment.os,
            request.environment.shell,
            request.environment.available_tools.join(", "),
            request.environment.current_directory,
            request.environment.git_repository.as_ref().unwrap_or(&"None".to_string()),
            request.constraints.join("\n- ")
        )
    }
}
```

### Real-time Command Suggestions

```rust
// examples/gemini_integration/src/realtime_assistant.rs
use tokio::sync::mpsc;
use tokio_stream::StreamExt;

pub struct RealtimeAssistant {
    gemini_client: GeminiClient,
    command_stream: mpsc::Receiver<CommandEvent>,
    suggestion_sender: mpsc::Sender<Suggestion>,
}

impl RealtimeAssistant {
    pub async fn start(&mut self) -> Result<()> {
        while let Some(event) = self.command_stream.recv().await {
            match event {
                CommandEvent::CommandStarted { command, context } => {
                    // Provide real-time suggestions as user types
                    if let Ok(suggestions) = self.get_quick_suggestions(&command, &context).await {
                        for suggestion in suggestions {
                            self.suggestion_sender.send(suggestion).await?;
                        }
                    }
                }
                CommandEvent::CommandCompleted { command, result, duration } => {
                    // Analyze completed commands for learning
                    self.analyze_completion(&command, &result, duration).await?;
                }
                CommandEvent::CommandFailed { command, error, exit_code } => {
                    // Provide immediate help for failures
                    if let Ok(help) = self.generate_error_help(&command, &error, exit_code).await {
                        self.suggestion_sender.send(Suggestion::ErrorHelp(help)).await?;
                    }
                }
            }
        }
        Ok(())
    }
    
    async fn get_quick_suggestions(&self, partial_command: &str, context: &CommandContext) -> Result<Vec<Suggestion>> {
        // Use Gemini's fast inference for real-time suggestions
        let prompt = format!(
            "User is typing: '{}'\nContext: {}\nProvide 3 quick completions or suggestions.",
            partial_command, context.summary()
        );
        
        let response = self.gemini_client.generate_content_fast(&prompt).await?;
        self.parse_suggestions(&response)
    }
}
```

### Gemini Code Review Integration

```bash
# Review code changes with Gemini
./gemini-analyzer review --git-diff

# Output:
# 📝 Gemini Code Review
# 
# **Files Changed**: 3 files, +47 -12 lines
# 
# **Overall Assessment**: ✅ Good changes with minor suggestions
# 
# **File: src/api/handlers.rs** (+23 -5)
# 
# ✅ **Positive**:
# - Good error handling with proper Result types
# - Clean separation of concerns
# - Well-structured async functions
# 
# ⚠️ **Suggestions**:
# ```rust
# // Line 42: Consider using a more specific error type
# // Instead of:
# return Err("Invalid input".into());
# 
# // Use:
# return Err(ApiError::InvalidInput { 
#     field: "user_id".to_string(),
#     value: input.user_id.clone()
# });
# ```
# 
# 💡 **Performance Note**:
# - Line 67: Database query in loop - consider batch processing
# - Use `futures::stream::iter().for_each_concurrent()` for parallel processing
# 
# **File: src/models/user.rs** (+15 -3)
# 
# ✅ **Excellent**:
# - Proper validation implementation
# - Good use of derive macros
# - Clear documentation
# 
# **File: tests/integration_tests.rs** (+9 -4)
# 
# ✅ **Testing Improvements**:
# - Good test coverage expansion
# - Proper async test structure
# 
# 🚀 **Additional Suggestions**:
# - Consider adding property-based tests with `proptest`
# - Mock external dependencies for faster tests
# - Add benchmark tests for performance regression detection
# 
# **Security Check**: ✅ No security issues detected
# **Performance Impact**: 🟡 Minor - database query optimization recommended
# **Maintainability**: ✅ High - clean, well-documented code
# 
# **Approval Recommendation**: ✅ APPROVED with minor suggestions
```

## Configuration

### Model Configuration

```toml
# gemini_config.toml
[gemini]
model = "gemini-1.5-pro-latest"  # Use latest for best performance
# model = "gemini-1.5-flash"     # Use flash for speed
temperature = 0.2                # Lower for more deterministic code analysis
max_output_tokens = 8192
top_p = 0.8
top_k = 40

[features]
vision_analysis = true           # Enable image analysis
code_generation = true           # Enable code generation
real_time_suggestions = false    # Enable real-time help (may use more tokens)
security_scanning = true         # Enable security analysis
performance_analysis = true      # Enable performance analysis

[safety]
harassment = "BLOCK_MEDIUM_AND_ABOVE"
hate_speech = "BLOCK_MEDIUM_AND_ABOVE"
sexually_explicit = "BLOCK_MEDIUM_AND_ABOVE"
dangerous_content = "BLOCK_MEDIUM_AND_ABOVE"

[analysis]
# Commands that trigger automatic analysis
auto_analyze_patterns = ["docker", "kubectl", "aws", "terraform", "git"]
# Minimum command length to analyze
min_command_length = 10
# Maximum tokens to use for context
max_context_tokens = 2000

[privacy]
# Sanitize before sending to API
sanitize_secrets = true
sanitize_paths = true
exclude_env_vars = ["PASSWORD", "SECRET", "TOKEN", "KEY", "PRIVATE"]
# Local patterns to redact
redact_patterns = [
    "sk-[a-zA-Z0-9]{48}",     # OpenAI API keys
    "ghp_[a-zA-Z0-9]{36}",    # GitHub tokens
    "xoxb-[0-9]{12}-[0-9]{12}-[a-zA-Z0-9]{24}"  # Slack tokens
]
```

## Best Practices

### 1. Efficient Token Usage

```rust
// Smart context management
pub struct ContextManager {
    max_tokens: usize,
    priority_weights: HashMap<String, f32>,
}

impl ContextManager {
    pub fn optimize_context(&self, commands: &[Command], current_context: &str) -> String {
        let mut context_parts = vec![];
        let mut token_count = 0;
        
        // Add current context (highest priority)
        if !current_context.is_empty() {
            context_parts.push(("current", current_context.to_string()));
            token_count += self.estimate_tokens(current_context);
        }
        
        // Add recent relevant commands
        for cmd in commands.iter().rev() {
            if token_count >= self.max_tokens * 0.8 {
                break;
            }
            
            if self.is_relevant_command(cmd, current_context) {
                let cmd_text = format!("{}: {}", cmd.timestamp, cmd.raw);
                context_parts.push(("history", cmd_text));
                token_count += self.estimate_tokens(&cmd.raw);
            }
        }
        
        self.format_context(&context_parts)
    }
}
```

### 2. Multi-Modal Analysis Pipeline

```rust
// Process screenshots alongside command data
pub struct MultiModalAnalyzer {
    gemini_client: GeminiClient,
    image_processor: ImageProcessor,
}

impl MultiModalAnalyzer {
    pub async fn analyze_with_screenshot(
        &self,
        command: &str,
        screenshot_path: &Path,
        context: &str,
    ) -> Result<Analysis> {
        // Process image to extract text and visual elements
        let image_data = self.image_processor.process_screenshot(screenshot_path).await?;
        
        // Combine text and image analysis
        let prompt = format!(
            "Analyze this command execution:\nCommand: {}\nContext: {}\nScreenshot shows: {}",
            command, context, image_data.extracted_text
        );
        
        let request = GeminiRequest {
            contents: vec![
                Content::text(prompt),
                Content::image(image_data.base64_data),
            ],
            generation_config: self.get_analysis_config(),
        };
        
        let response = self.gemini_client.generate_content(request).await?;
        Ok(self.parse_analysis_response(response)?)
    }
}
```

### 3. Continuous Learning System

```rust
// Learn from user feedback to improve suggestions
pub struct LearningSystem {
    feedback_store: FeedbackStore,
    model_fine_tuner: ModelFineTuner,
}

impl LearningSystem {
    pub async fn record_feedback(&self, suggestion_id: &str, feedback: UserFeedback) -> Result<()> {
        self.feedback_store.store(suggestion_id, feedback).await?;
        
        // Trigger model updates if enough feedback accumulated
        if self.should_retrain().await? {
            self.schedule_retraining().await?;
        }
        
        Ok(())
    }
    
    pub async fn get_personalized_suggestions(&self, context: &CommandContext) -> Result<Vec<Suggestion>> {
        // Use historical feedback to weight suggestions
        let base_suggestions = self.generate_base_suggestions(context).await?;
        let user_preferences = self.analyze_user_preferences().await?;
        
        Ok(self.rank_suggestions(base_suggestions, user_preferences))
    }
}
```

## Troubleshooting

### Common Issues

1. **API Quota Exceeded**
   ```bash
   # Check quota status
   ./gemini-analyzer quota status
   
   # Enable smart batching
   export GEMINI_BATCH_REQUESTS=true
   ```

2. **Large Context Windows**
   ```bash
   # Optimize context size
   ./gemini-analyzer config set max_context_tokens 4000
   
   # Enable context compression
   ./gemini-analyzer config set compress_context true
   ```

3. **Vision Analysis Issues**
   ```bash
   # Test image processing
   ./gemini-analyzer vision test --image screenshot.png
   
   # Check supported formats
   ./gemini-analyzer vision formats
   ```

## Next Steps

- Compare with [Claude Integration](../claude_integration/) for analysis differences
- Explore [Multi-AI Analysis](../multi_ai_analysis/) for combined insights
- Learn about [Privacy Considerations](../privacy/) for sensitive environments

---

**Note**: This example demonstrates integration patterns. Implement actual API calls using the official Google AI SDK.